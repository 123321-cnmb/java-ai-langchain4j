<img width="3894" height="2693" alt="image" src="https://github.com/user-attachments/assets/27ea6cbb-ed28-4c33-b95a-dd5e047978c7" />


这个项目是一个**基于大语言模型的智能医院客服系统**。它不仅仅是一个简单的聊天机器人，而是一个能够查询医疗知识、记住用户对话、甚至能通过调用后台接口执行“预约挂号”任务的**智能 Agent（代理）**。

### 项目技术栈

 Spring Boot + LangChain4j+通义千问 (Qwen) 流式模型 + MongoDB +Pinecone (分布式向量存储) + MySQL + MyBatis-Plus

## 第一阶段：核心对话
项目的入口是 `XiaozhiController.java`
* **流式响应 (Streaming)**：项目采用了 `Flux<String>` 返回类型。这意味着 AI 的回复不是等全部写完才发给用户，而是像“打字机”一样，出一字发一字，极大地提升了用户体验。
* **角色设定**：通过 `@SystemMessage` 引入了 `zhaozhi-prompt-template.txt` 资源文件。这相当于给 AI 下达了“指令集”，规定它必须扮演一个“医院智能客服”的角色，而不是通用百科全书。

## 第二阶段：持久化记忆
普通的 AI 对话是“无状态”的（问完即忘）。本项目通过 `MongoChatMemoryStore` 解决了这个问题。
* **滑动窗口记忆**：在 `XiaozhiAgentConfig` 中配置了 `MessageWindowChatMemory`，只保留最近的 20 条消息。这样既能让 AI 联系上下文，又不会因为对话太长导致消耗过多的 Token。
* **数据库存储**：利用 **MongoDB** 存储 `ChatMessages` 实体。即使服务器重启，只要传入相同的 `memoryId`，AI 就能从 MongoDB 中读取之前的 JSON 记录并还原对话现场。

## 第三阶段：RAG 知识库
为了让它掌握医院内部的专业信息（如神经内科的具体位置），项目引入了 **RAG（检索增强生成）** 流程。
* **向量检索**：项目使用了 **Pinecone**（云端向量数据库）作为知识存储。
* **工作原理**：
1. 系统预先将医院的 `.md` 文档转化为向量数据。
2. 当用户提问时，`contentRetrieverXiaozhiPincone` 会去数据库匹配相似度大于 **0.8** 的知识片段。
3. 系统将这些真实文档和用户问题一起发给 AI，AI 基于这些“参考资料”给出准确回答。

## 第四阶段：工具调用
**Function Calling（工具调用）**。AI 除了会聊天，还能操作业务系统。
* **业务联动**：在 `XiaozhiAgent` 的注解中配置了 `tools = "appointmentTools"`。
* **执行逻辑**：
* 如果用户说：“我想查一下我在神经内科的预约”。
* AI 会识别出意图，自动调用 `AppointmentService` 中的方法。
* 系统会去 **MySQL** 数据库中查询 `Appointment` 表。
* AI 拿到数据库返回的结果后，再组织成自然语言告诉用户

### 第五阶段：多模态实时语音通话

本项目在文本 Agent 的基础上，通过集成阿里云语音技术栈（ASR & TTS）与 WebSocket 全双工通信，实现了如同拨打电话般的实时语音交互功能。

#### 1. 全双工语音流处理 (`CallHandler.java`)

项目不再依赖传统的“录音-上传-处理”模式，而是采用了基于 **WebSocket** 的实时二进制流处理架构。

* **实时听觉 (ASR)**：利用阿里云 `SpeechTranscriber`，将前端麦克风采集的 PCM 音频流实时转化为文本。通过设置 `max_sentence_silence` 参数（800ms），系统能自动感知用户说话结束并触发 AI 思考。
* **半双工状态锁**：为了防止 AI 播报的声音被麦克风重新录入造成“自干扰”，后端引入了 `isAiSpeaking` 原子锁。当 AI 处于思考或播报状态时，系统会主动拦截并丢弃用户的音频输入，确保交互逻辑的严密性。

#### 2. 顺序化思考与播报逻辑

为了保证医疗场景下回答的严谨性，项目采用了“**完整生成-阻塞播报**”的策略：

* **思考展示**：当 ASR 识别结束，系统进入 `AI_THINKING` 状态。此时大模型（Qwen）开始根据上下文及 RAG 知识库生成回答。为了缓解用户等待焦虑，生成的文字会通过 WebSocket 实时推送到前端展示。
* **缓冲播报**：系统会利用 `blockLast()` 确保大模型完成全部逻辑推理。在文本完全生成后，系统可根据配置进行 5 秒的预留缓冲，随后调用 `VoiceService` 进行整段语音合成，彻底解决了流式语音常见的声音断断续续问题。

#### 3. 语音合成与下发 (`VoiceService.java`)

* **高保真合成 (TTS)**：系统调用阿里云 `SpeechSynthesizer` 接口。对于长文本，系统支持一次性合成完整的音频字节流（byte[]），并通过 WebSocket 的 `BinaryMessage` 形式一次性下发至前端。
* **音频格式优化**：采用 MP3 格式进行传输，兼顾了高压缩比与浏览器的原生播放兼容性。

#### 4. 前端交互体验优化

* **流式音频队列**：前端采用 `Blob` 机制接收后端传回的二进制音频大块。通过 `currentAudio` 对象管理，确保在 AI 开始新的播报前自动静音上一段音频，实现无缝的语音切换。
* **状态反馈视觉化**：前端 UI 会根据后端下发的 `STATE` 指令（如 `AI_THINKING`、`AI_SPEAKING`）动态改变输入框边框颜色和占位符文字，向用户明确传达系统当前处于“思考中”还是“播报中”。



